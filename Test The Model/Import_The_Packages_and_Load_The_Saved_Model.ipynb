{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "5e7fc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "bb3c723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e7e7f",
   "metadata": {},
   "source": [
    "# MODEL BUILDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8f8b9",
   "metadata": {},
   "source": [
    "Adding The Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "3d0ddfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "1d77577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "0e136b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Datagen\n",
    "train_datagen = ImageDataGenerator(rescale=1/255,zoom_range=0.2,horizontal_flip=True,vertical_flip=False)\n",
    "# Testing Datagen\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "dadf473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15750 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train=train_datagen.flow_from_directory('/content/Dataset/training_set',target_size=(64,64), class_mode='categorical',batch_size=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "f1b5fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2250 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "x_test=test_datagen.flow_from_directory('/content/Dataset/test_set',target_size=(64,64),batch_size=200,\n",
    "                                          class_mode='categorical',color_mode=\"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "ee809657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len x-train :  18\n"
     ]
    }
   ],
   "source": [
    "print(\"Len x-train : \", len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "849e034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len x-test :  12\n"
     ]
    }
   ],
   "source": [
    "print(\"Len x-test : \", len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "3f4a8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "9cf9bd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  2]\n",
      " [-2 -2]] is a kernel for detecting horizontal edges\n",
      "[[ 2 -2]\n",
      " [ 2 -2]] is a kernel for detecting vertical edges\n"
     ]
    }
   ],
   "source": [
    "# let img1 be an image with no features\n",
    "img1 = np.array([np.array([200, 200]), np.array([200, 200])])\n",
    "img2 = np.array([np.array([200, 200]), np.array([0, 0])])\n",
    "img3 = np.array([np.array([200, 0]), np.array([200, 0])])\n",
    "  \n",
    "kernel_horizontal = np.array([np.array([2, 2]), np.array([-2, -2])])\n",
    "print(kernel_horizontal, 'is a kernel for detecting horizontal edges')\n",
    "  \n",
    "kernel_vertical = np.array([np.array([2, -2]), np.array([2, -2])])\n",
    "print(kernel_vertical, 'is a kernel for detecting vertical edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "66cc1a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAExElEQVR4nO3bzculcxzH8c93QpOUp5SIyHJWFrMYSTbKRhayQik27PwDTLLwD1iYnZRMU5LdjI1iQzJZKLMbUVJYejY/i/ume3E3MQ/3+czt9apT51xd5+p7Oufd7zpPs9YK0OfApgcAdidOKCVOKCVOKCVOKCVOKCXOUjPzxcw8uOk52BxxllprHVprfXCpjzszx2bmzMycm5mnL/XxuXTE+f/zeZLnk3y26UE4v6s2PQC7m5mzSZ5Ncn+SQ0l+TfJokrNJHtu+vLC9/Zm11qnt+92d5I0k9yb5OMmZJNevtZ5MkrXWa9v7/bJ3j4YLYeW8MjyS5M0kNyY5neRktp6725O8nOT1Hfu+leSTJDcnOZrkqb0clEtHnFeGD9daJ9dafyQ5keSWJK+utX5P8naSu2bmhpm5M8nhJC+utX5ba32U5L3Njc3FEOeV4bsd139O8v1a688dt5PkuiS3JflxrfXTjv2/3oP5uAzEub98m+Smmbl2x7Y7NjUMF0ec+8ha66sknyY5OjPXzMyRbL1f/cf29oNJJsnVM3NwZrwOCnlS9p8nkhxJ8kOSV5Icz9Ynun87la1T4fuSHNu+/sAez8i/MP5svb/NzPEkX661Xtr0LPw3Vs59ZmYOz8w9M3NgZh7O1nej7254LC6AHyHsP7cmeSdb33N+k+S5tdbpzY7EhXBaC6Wc1kKp857WPnTgccsqXGbvnzsxu223ckIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUIpcUKpWWttegZgF1ZOKCVOKCVOKCVOKCVOKCVOKPUXV4959KCnMpoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizontal edge confidence score: 0\n",
      "Vertical edge confidence score: 0\n"
     ]
    }
   ],
   "source": [
    "def apply_kernel(img, kernel):\n",
    "    return np.sum(np.multiply(img, kernel))\n",
    "  \n",
    "# Visualizing img1\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')\n",
    "plt.title('img1')\n",
    "plt.show()\n",
    "\n",
    "# Checking for horizontal and vertical features in image1\n",
    "print('Horizontal edge confidence score:', apply_kernel(img1, \n",
    "                                            kernel_horizontal))\n",
    "print('Vertical edge confidence score:', apply_kernel(img1, \n",
    "                                            kernel_vertical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "d6bef2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFB0lEQVR4nO3azavmcxjH8c81yUKM58hTSVjMhsUsRLIRG0mWWIiNpbKwI/EvKEuRaTbCDhsbC1JWipSIQswsJLLga3EfdZqOyWhm7o/j9aq7zvmd3/nd193p3XU/nFlrBehzYNsDAHsTJ5QSJ5QSJ5QSJ5QSJ5QSZ6mZ+WRm7tr2HGyPOEuttQ6ttd47ndecmZtm5s2Z+WFmjs/M2zNz8+m8D04fcf6/XJTkrSQ3J7kiyYdJ3tzmQPy98R9CnWbmyySPJ7kjyaEkvyW5P8mXSR7cuT25c/yxtdY7O793fZKXk9ya5IMknyW5cK318B73cUmSY0kuW2sdO7OPiFNlc/433JfklSQXJ/k4ydvZ/O2uTvJckpd2nftaNhvx0iTPJnnkJNe9M8l3wuxkc5Y6YXPevta6e+f4fUmOZLMNf5+ZC5L8lE24B5N8keTgWuuXnfNfTZITN+fMXJPNZn1qrXXkrDwoTonN+d/w/a6vf03y41rr913fJ8n5Sa5KcvyvMHd8feLFZubyJO8keVGYvcS5v3yb5JKZOW/XsWt3nzAzF2cT5ltrrRfO5nCcGnHuI2utr5J8lOTZmTl3Zm7L5vVqkmRmDmbzevX9tdbTWxqTf0ic+89DSW7L5l3Y55MczeYd3SR5IMnhJI/OzM+7btdtZ1ROxhtC+9zMHE3y6VrrmW3PwqmxOfeZmTk8MzfMzIGZuTebz0bf2PJY/AvnbHsATrsrk7yezeec3yR5Yq318XZH4t/wtBZKeVoLpU76tPaP7260VuEMO3Dl57Pn8bM9CPDPiBNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKiRNKnXOyH95z1S1naQz4/3r3j72P25xQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQSpxQatZa254B2IPNCaXECaXECaXECaXECaXECaX+BHW+mUCWKS4/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizontal edge confidence score: 800\n",
      "Vertical edge confidence score: 0\n"
     ]
    }
   ],
   "source": [
    "# Visualizing img2\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')\n",
    "plt.title('img2')\n",
    "plt.show()\n",
    "\n",
    "# Checking for horizontal and vertical features in image2\n",
    "print('Horizontal edge confidence score:', apply_kernel(img2, \n",
    "                                            kernel_horizontal))\n",
    "print('Vertical edge confidence score:', apply_kernel(img2, \n",
    "                                            kernel_vertical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "9bf9b5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFOUlEQVR4nO3awYudZxnG4fuJWqNopFFUUqTYopsu2o2gJAsXagsliGt1141/QeuqoX9CwYVQSouhIQRE3IkuRBBpEQKGQqUgkYjVYEQUbKAmbxczgSEOCRknOXfG61rNfOc73zyH4cf7nu+cWWsF6HNo0wMAuxMnlBInlBInlBInlBInlBJnqZl5c2a+uuk52BxxllprPbbW+uV+XnNmPjUzv56ZKzPzj5n5zcwc38+/wf4ZX0L4/zEzh5M8nOTtJCvJN5O8nOTTa63/bHI2/puVs9TMXJyZr83MqZk5NzOnZ+ZfM3NhZr44M9+fmcszc2lmvrHjeZ+fmV9tn/uLmfnBzJxOkrXW1bXW79da15NMkmtJHkxydDOvklsR5/3hZJIfZSuk80l+lq3/3UNJXkjywx3nvpbkjSSfTHIqyXdvvtjM/C7J1SQ/TfLSWuvyXZydPbKtLTUzF5M8k+REkuNrra9vHz+Z5EyST6y1rs3Mx5P8M1vhHknyhyRH1lr/3j7/xqr5nZuufzjJt5I8sNZ69Z68KO6IlfP+8NcdP7+b5G9rrWs7fk+SjyU5luTvN8Lcdmm3C25vcc8keW5mHt/vgfnfifNgeSfJ0Zn56I5jn7vNcz6U5JG7NxJ7Jc4DZK31xyS/TXJqZh6Yma9k6/1qkmRmvjwzJ7Yf+8jMPJvkM0le39DI3MIHNz0A++7bSV5JciVbN4bOJvnA9mMfTvJitlbK95JcSPL0WuvP935MbscNoQNuZs4meWut9fymZ+HO2NYeMDPzpZl5dGYOzcxT2fqiwU82PBZ7YFt78Hw2yY+z9Tnnn5J8b611frMjsRe2tVDKthZK3XJbe/0vX7Cs3meePPbEpkfgDv38+rnZ7biVE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0qJE0rNWmvTMwC7sHJCKXFCKXFCKXFCKXFCKXFCqfcBLDufqvQo0psAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizontal edge confidence score: 0\n",
      "Vertical edge confidence score: 800\n"
     ]
    }
   ],
   "source": [
    "# Visualizing img3\n",
    "plt.imshow(img3)\n",
    "plt.axis('off')\n",
    "plt.title('img3')\n",
    "plt.show()\n",
    "  \n",
    "# Checking for horizontal and vertical features in image3\n",
    "print('Horizontal edge confidence score:', apply_kernel(img3, \n",
    "                                            kernel_horizontal))\n",
    "print('Vertical edge confidence score:', apply_kernel(img3, \n",
    "                                            kernel_vertical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c041ad75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Class Indices in Training Dataset\n",
    "x_train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "214c30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b02306f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "882aab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef0dd4",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "3b2776b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd1999",
   "metadata": {},
   "source": [
    "# Adding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "2d67694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.add(Conv2D(33,(3,3),input_shape=(64,64,1), activation='relu'))\n",
    "\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab6c1b",
   "metadata": {},
   "source": [
    "# 1st hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "d5460052",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(400,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ce77b",
   "metadata": {},
   "source": [
    "# 2nd hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "0095e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(200,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f2e06",
   "metadata": {},
   "source": [
    "# 3rd hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "e868e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(100,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02181247",
   "metadata": {},
   "source": [
    "# output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "e2b301d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(9,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a581b49",
   "metadata": {},
   "source": [
    "# Compile The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8d91af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fb8b6",
   "metadata": {},
   "source": [
    "# Fit The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "bd1ea460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harini\\AppData\\Local\\Temp/ipykernel_13792/4028576845.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(x_train,steps_per_epoch=30,epochs=10,validation_data=x_test,validation_steps=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/30 [=================>............] - ETA: 57s - loss: 1.2575 - accuracy: 0.5933 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
      "30/30 [==============================] - 107s 3s/step - loss: 1.2575 - accuracy: 0.5933 - val_loss: 0.4059 - val_accuracy: 0.9040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x238f9346100>"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(x_train,steps_per_epoch=30,epochs=10,validation_data=x_test,validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "80bc9767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.2315 - accuracy: 0.9316 - val_loss: 0.2413 - val_accuracy: 0.9338\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 52s 3s/step - loss: 0.0777 - accuracy: 0.9781 - val_loss: 0.2032 - val_accuracy: 0.9556\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.0362 - accuracy: 0.9909 - val_loss: 0.1937 - val_accuracy: 0.9707\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.0213 - accuracy: 0.9950 - val_loss: 0.1634 - val_accuracy: 0.9742\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.0133 - accuracy: 0.9972 - val_loss: 0.1722 - val_accuracy: 0.9764\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 51s 3s/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 0.1862 - val_accuracy: 0.9764\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 52s 3s/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.2045 - val_accuracy: 0.9773\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 52s 3s/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 0.2209 - val_accuracy: 0.9778\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.1856 - val_accuracy: 0.9769\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.1958 - val_accuracy: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x238b9e39e50>"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,epochs=10,validation_data=x_test,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979983ed",
   "metadata": {},
   "source": [
    "# Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "86e99622",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('aslpng1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "f54621c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-win_amd64.whl (35.6 MB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\harini\\anaconda3\\lib\\site-packages (from opencv-python) (1.23.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85cd839",
   "metadata": {},
   "source": [
    "# Import the packages and load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "48bb8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "99cc65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a7dc4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('aslpng1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "0b2a6f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x400 at 0x238CB2CBB20>"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=image.load_img('/content/Dataset/test_set/F/10.png',target_size=(400,500))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9b795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
